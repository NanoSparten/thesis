
% paper dating analysis
@inproceedings{lu_paper_2021,
  title     = {Paper {Dating} {Analysis} {Based} on {Paper} {Texture} {Image}
  {Feature}},
  doi       = {10.1109/ICoIAS53694.2021.00010},
  abstract  = {Paper
  dating analysis is an important research direction of document inspection,
  which is widely used in the inspection and identification of cultural relics
  and ancient books. This article focuses on the analysis of paper dating in
  ancient books, and proposes a non-destructive inspection and analysis method
  based on paper fiber texture images. Aiming at the global stacking morphology
  of paper fibers and the local features of specific types of fiber morphology,
  we propose a neural network-based hybrid texture feature extraction and
  representation method: On the one hand, we use convolutional networks to
  obtain global features of fiber texture; at the same time, we designed a
  method of extracting and representing local fiber morphological features based
  on the attention mechanism. By mixing the above two types of features, we
  realize the extraction and representation of paper fiber features.
  Furthermore, we use the GRU(Gate Recurrent Unit) model to establish a paper
  dating time series model and design a new loss function. In order to verify
  the method, this paper selects 36 domestic books published from 1950 to 2000,
  and uses the document checker VSC 6000 to collect paper texture images as a
  dataset, and verifies the effectiveness of the proposed method on this
  dataset. Experiments prove that the method proposed in this paper has achieved
  ideal results in paper dating analysis.},
  booktitle = {2021 4th
  {International} {Conference} on {Intelligent} {Autonomous} {Systems}
  ({ICoIAS})},
  author    = {Lu, Qi and Zhu, Ziqi and Li, Zhihao and Lian, Zhe},
  month     = may,
  year      = {2021},
  keywords  = {Time series analysis,
  Stacking, Neural networks, Layout, Morphology, Production, Optical fiber
  networks, paper dating analysis, neural network, regression model, attention
  mechanism, GRU},
  pages     = {13--17}
}